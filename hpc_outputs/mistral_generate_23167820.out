Namespace(note='default', api='vllm', seed=42, verbose=False, wandb_mode='disabled', model_ckpt='mistralai/Mistral-7B-v0.1', model_parallel=False, half_precision=False, max_tokens=1024, temperature=0.8, top_k=40, top_p=0.95, num_beams=1, max_num_worker=3, test_batch_size=1, tensor_parallel_size=1, prompts_root='prompts', data_root='data', dataset_name='GSM8K', test_json_filename='1218_to_1318', start_idx=0, end_idx=inf, run_outputs_root='run_outputs', eval_outputs_root='eval_outputs', num_rollouts=32, num_subquestions=3, num_votes=10, max_depth_allowed=5, mcts_discount_factor=1.0, mcts_exploration_weight=2.0, mcts_weight_scheduler='const', mcts_num_last_votes=32, save_tree=False, num_a1_steps=3, disable_a1=False, modify_prompts_for_rephrasing=False, disable_a5=False, enable_potential_score=False, fewshot_cot_prompt_path='prompts/GSM8K/fewshot_cot/fewshot_cot_prompt.txt', fewshot_cot_config_path='prompts/GSM8K/fewshot_cot/fewshot_cot_config.json', fewshot_ost_prompt_path='prompts/GSM8K/fewshot_ost/fewshot_ost_prompt.txt', fewshot_ost_config_path='prompts/GSM8K/fewshot_ost/fewshot_ost_config.json', decompose_template_path='prompts/GSM8K/decompose/decompose_template.json', decompose_prompt_path='prompts/GSM8K/decompose/decompose_prompt.txt', rephrasing_prompt_template_path='prompts/GSM8K/rephrasing_prompt_template.txt', fewshot_cot_prompt_rephrased_path='prompts/GSM8K/fewshot_cot/fewshot_cot_prompt.txt', fewshot_ost_prompt_rephrased_path='prompts/GSM8K/fewshot_ost/fewshot_ost_prompt.txt', decompose_prompt_rephrased_path='prompts/GSM8K/decompose/decompose_prompt.txt', run_outputs_dir='run_outputs/GSM8K/Mistral-7B-v0.1/2024-11-14_12-26-17---[default]', answer_sheets_dir='run_outputs/GSM8K/Mistral-7B-v0.1/2024-11-14_12-26-17---[default]/answer_sheets', cuda_0='NVIDIA A100 80GB PCIe', cuda_1=None, cuda_2=None, cuda_3=None)
INFO 11-14 12:26:24 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=mistralai/Mistral-7B-v0.1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-14 12:26:25 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.
INFO 11-14 12:26:25 selector.py:115] Using XFormers backend.
INFO 11-14 12:26:26 model_runner.py:1056] Starting to load model mistralai/Mistral-7B-v0.1...
INFO 11-14 12:26:26 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.
INFO 11-14 12:26:26 selector.py:115] Using XFormers backend.
INFO 11-14 12:26:28 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-14 12:26:38 model_runner.py:1067] Loading model weights took 13.4966 GB
INFO 11-14 12:26:40 gpu_executor.py:122] # GPU blocks: 27498, # CPU blocks: 8192
INFO 11-14 12:26:40 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 13.43x
INFO 11-14 12:26:49 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-14 12:26:49 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-14 12:27:02 model_runner.py:1523] Graph capturing finished in 14 secs.
==> Total calls: 15321, Avg calls: 151.69
==> Total tokens: 29706506, Avg tokens: 294123.82
==> Total time: 39235.29s, Avg time: 388.47s

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23167820: <mistral_generate> in cluster <dcc> Done

Job <mistral_generate> was submitted from host <hpclogin1> by user <s233498> in cluster <dcc> at Thu Nov 14 11:37:12 2024
Job was executed on host(s) <4*n-62-18-13>, in queue <gpua100>, as user <s233498> in cluster <dcc> at Thu Nov 14 12:26:05 2024
</zhome/e3/f/203690> was used as the home directory.
</dtu/blackhole/0a/203690/rStar> was used as the working directory.
Started at Thu Nov 14 12:26:05 2024
Terminated at Thu Nov 14 23:21:02 2024
Results reported at Thu Nov 14 23:21:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q gpua100 
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -J mistral_generate
#BSUB -n 4
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=40GB]"
#BSUB -W 14:00
#BSUB -o hpc_outputs/mistral_generate_%J.out
#BSUB -e hpc_outputs/mistral_generate_%J.err


#### Set environment variables #### 
export HUGGING_FACE_HUB_TOKEN=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
export HUGGINGFACE_TOKEN=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

export HF_HOME=/dtu/blackhole/0a/203690/huggingface_cache

source /dtu/blackhole/0a/203690/miniconda3/bin/activate

conda activate rStar_new

CUDA_VISIBLE_DEVICES=0 python run_src/do_generate.py \
    --dataset_name GSM8K \
    --test_json_filename 1218_to_1318 \
    --model_ckpt mistralai/Mistral-7B-v0.1 \
    --note default \
    --num_rollouts 32 

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39676.00 sec.
    Max Memory :                                 3187 MB
    Average Memory :                             2670.38 MB
    Total Requested Memory :                     163840.00 MB
    Delta Memory :                               160653.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                35
    Run time :                                   39298 sec.
    Turnaround time :                            42230 sec.

The output (if any) is above this job summary.



PS:

Read file <hpc_outputs/mistral_generate_23167820.err> for stderr output of this job.

