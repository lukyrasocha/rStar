/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
Traceback (most recent call last):
  File "/dtu/blackhole/17/209207/DL_project/rStar/run_src/do_discriminate.py", line 576, in <module>
    main()
  File "/dtu/blackhole/17/209207/DL_project/rStar/run_src/do_discriminate.py", line 393, in main
    discriminator = MajorityVoteDiscriminator(args, evaluator)
  File "/dtu/blackhole/17/209207/DL_project/rStar/run_src/do_discriminate.py", line 319, in __init__
    self.tokenizer, self.model = load_vLLM_model(args.model_ckpt, args.seed, max_num_seqs=args.max_num_seqs)
  File "/dtu/blackhole/17/209207/DL_project/rStar/./models/vLLM_API.py", line 23, in load_vLLM_model
    llm = LLM(
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 177, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 573, in from_engine_args
    engine = cls(
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 334, in __init__
    self.model_executor = executor_class(
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 47, in __init__
    self._init_executor()
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 39, in _init_executor
    self.driver_worker.init_device()
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/worker/worker.py", line 168, in init_device
    _check_if_gpu_supports_dtype(self.model_config.dtype)
  File "/dtu/blackhole/17/209207/miniconda3/envs/DL_project/lib/python3.10/site-packages/vllm/worker/worker.py", line 473, in _check_if_gpu_supports_dtype
    raise ValueError(
ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla V100-PCIE-32GB GPU has compute capability 7.0. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.
